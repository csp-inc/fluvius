{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44598148",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Environment setup\n",
    "from pystac_client import Client\n",
    "import planetary_computer as pc\n",
    "import os\n",
    "\n",
    "# Set the environment variable PC_SDK_SUBSCRIPTION_KEY, or set it here.\n",
    "# The Hub sets PC_SDK_SUBSCRIPTION_KEY automatically.\n",
    "# pc.settings.set_subscription_key(<YOUR API Key>)\n",
    "#env_vars = !cat /content/.env\n",
    "env_vars = open(\"/content/.env\",\"r\").read().split('\\n')\n",
    "for var in env_vars[:-1]:\n",
    "    key, value = var.split(' = ')\n",
    "    os.environ[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1f1f3f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import fsspec\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio import windows\n",
    "from rasterio import features\n",
    "from rasterio import warp\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "#experiments with dask\n",
    "import dask.dataframe as dd\n",
    "import dask_geopandas as dg\n",
    "\n",
    "container = 'ana-data'\n",
    "\n",
    "storage_options={'account_name':os.environ['ACCOUNT_NAME'],\\\n",
    "                 'account_key':os.environ['BLOB_KEY']}\n",
    "fs = fsspec.filesystem('az', account_name=storage_options['account_name'], account_key=storage_options['account_key'])\n",
    "station_url = f'az://{container}/ana_station_metadata.csv'\n",
    "station_df = pd.read_csv(station_url, storage_options=storage_options)                \n",
    "#df = dd.read_csv(station_url, storage_options=storage_options)\n",
    "#ddf = dg.from_dask_dataframe(df).set_geometry(dg.points_from_xy(ddf, 'Latitude', 'Longitude'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c3d64742",
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy from old data\n",
    "#csp_storage_options={'account_name':os.environ['CSP_ACCOUNT_NAME'],\\\n",
    "#                 'account_key':os.environ['CSP_BLOB_KEY']}\n",
    "#csp_fs = fsspec.filesystem('az',\\\n",
    "#                           account_name=csp_storage_options['account_name'],\\\n",
    "#                           account_key=csp_storage_options['account_key'])\n",
    "#for i in csp_fs.ls('ana-data'):\n",
    "#    f = pd.read_csv(f'az://{i}', delimiter=';', storage_options=csp_storage_options)\n",
    "#    f.to_csv(f'az://{i}', storage_options=storage_options, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705a3f10",
   "metadata": {},
   "source": [
    "# Here we preprocessing of the data for standardized ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3f2a2831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing to az://ana-data/stations/21750000.csv\n",
      "writing to az://ana-data/stations/22050001.csv\n",
      "writing to az://ana-data/stations/22500000.csv\n",
      "writing to az://ana-data/stations/22700000.csv\n",
      "writing to az://ana-data/stations/22900000.csv\n",
      "writing to az://ana-data/stations/23100000.csv\n",
      "writing to az://ana-data/stations/23230000.csv\n",
      "writing to az://ana-data/stations/23250000.csv\n",
      "writing to az://ana-data/stations/23700000.csv\n",
      "writing to az://ana-data/stations/25200000.csv\n",
      "writing to az://ana-data/stations/26200000.csv\n",
      "writing to az://ana-data/stations/26300000.csv\n",
      "writing to az://ana-data/stations/26350000.csv\n",
      "writing to az://ana-data/stations/26800000.csv\n",
      "writing to az://ana-data/stations/27500000.csv\n",
      "writing to az://ana-data/stations/27550000.csv\n",
      "writing to az://ana-data/stations/28240000.csv\n",
      "writing to az://ana-data/stations/28300000.csv\n",
      "writing to az://ana-data/stations/28850000.csv\n",
      "writing to az://ana-data/stations/29050000.csv\n",
      "writing to az://ana-data/stations/29100000.csv\n",
      "writing to az://ana-data/stations/29200000.csv\n"
     ]
    }
   ],
   "source": [
    "for site_no in station_df.site_no:\n",
    "    station_url = f'az://{container}/{site_no}.csv'\n",
    "    station_url2 = f'az://{container}/{site_no}_2.csv'\n",
    "    site_df1_raw = pd.read_csv(station_url, delimiter=',', skiprows=10, storage_options=storage_options)\n",
    "    translation = pd.read_csv(f'az://{container}/ana_translations.csv', storage_options=storage_options)\n",
    "    trans = {p:e  for p,e in zip(translation.Portuguese, translation.English)}\n",
    "    site_df1 = site_df1_raw.rename(columns=trans)\n",
    "    site_df1 = site_df1.dropna(subset=['Date'])\n",
    "    site_df1['TimeL'] = site_df1['TimeL'].fillna('01/01/1900 01:00')\n",
    "    site_df1['Date-Time'] = [d for d in site_df1['Date']]\n",
    "    site_df1['Date-Time'] = pd.to_datetime(site_df1['Date-Time'],\\\n",
    "                                      format='%d/%m/%Y')\n",
    "\n",
    "    site_df2_raw = pd.read_csv(station_url2, delimiter=',', skiprows=14, storage_options=storage_options)\n",
    "    site_df2_raw = site_df2_raw.replace('01/01/1900', '01/01/1900 01:00')\n",
    "    translation2 = {'Data':'Date','Hora':'Hour','Turbidez':'Turbidity'}\n",
    "    site_df2 = site_df2_raw.rename(columns=translation2)\n",
    "    site_df2 = site_df2.dropna(subset=['Date'])\n",
    "    site_df2['Date-Time-HM'] = [f\"{d} {t.split(' ')[1]}\" for d,t in zip(site_df2['Date'],site_df2['Hour'])]\n",
    "    site_df2['Date-Time'] = [d for d in site_df2['Date']]\n",
    "    site_df2['Date-Time'] = pd.to_datetime(site_df2['Date-Time'],\\\n",
    "                                      format='%d/%m/%Y')\n",
    "    site_df2 = site_df2[['Date', 'Hour', 'Date-Time','Turbidity']]\n",
    "\n",
    "    selection = ['Date-Time', 'Discharge', 'Suspended Sediment Concentration (mg/L)', 'Turbidity']\n",
    "    site_df = site_df1.merge(site_df2, on='Date', how='outer', suffixes=('_',''))\n",
    "    site_df['Date-Time'] = site_df['Date-Time'].fillna(site_df['Date-Time_'])\n",
    "    #site_df['Hour'] = site_df['Hour'].fillna(site_df['Hour_'])\n",
    "    site_df = site_df[selection]\n",
    "    s = str(site_no).zfill(8)\n",
    "    write_filename = f'az://{container}/stations/{str(site_no)}.csv'\n",
    "    print(f'writing to {write_filename}')\n",
    "    site_df.to_csv(write_filename, index=False, storage_options=storage_options)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
