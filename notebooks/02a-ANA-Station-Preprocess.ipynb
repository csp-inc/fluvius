{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44598148",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Environment setup\n",
    "from pystac_client import Client\n",
    "import planetary_computer as pc\n",
    "import os\n",
    "\n",
    "# Set the environment variable PC_SDK_SUBSCRIPTION_KEY, or set it here.\n",
    "# The Hub sets PC_SDK_SUBSCRIPTION_KEY automatically.\n",
    "# pc.settings.set_subscription_key(<YOUR API Key>)\n",
    "env_vars = !cat /content/.env\n",
    "\n",
    "for var in env_vars:\n",
    "    key, value = var.split(' = ')\n",
    "    os.environ[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1f1f3f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nstation_list = pd.DataFrame({'site_no':filter(None,                           map(lambda sub:(''.join([ele for ele in sub if ele.isnumeric()])), fs_list))}).astype('int')\\nfiltered_station_df = station_list.merge(station_df, on='site_no')\\n#filter the stations to ones with data\\n\\ncrs = {'init': 'epsg:4326'}\\ngdf = gpd.GeoDataFrame(filtered_station_df,                       geometry=gpd.points_from_xy(filtered_station_df.Longitude, filtered_station_df.Latitude))\\nbuffer_style = {'round':1,'flat':2,'square':3}\\nbuffer = 0.025\\ngdf.geometry = gdf.geometry.buffer(buffer, cap_style = buffer_style['square'], resolution=1)\\n\\ngdf = gdf.drop_duplicates()\\n\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import fsspec\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio import windows\n",
    "from rasterio import features\n",
    "from rasterio import warp\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "#experiments with dask\n",
    "import dask.dataframe as dd\n",
    "import dask_geopandas as dg\n",
    "\n",
    "container = 'ana-data'\n",
    "\n",
    "storage_options={'account_name':os.environ['ACCOUNT_NAME'],\\\n",
    "                 'account_key':os.environ['BLOB_KEY']}\n",
    "fs = fsspec.filesystem('az', account_name=storage_options['account_name'], account_key=storage_options['account_key'])\n",
    "station_url = f'az://{container}/ana_station_metatdata.csv'\n",
    "station_df = pd.read_csv(station_url, storage_options=storage_options)                \n",
    "df = dd.read_csv(station_url, storage_options=storage_options)\n",
    "#ddf = dg.from_dask_dataframe(df).set_geometry(dg.points_from_xy(ddf, 'Latitude', 'Longitude'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705a3f10",
   "metadata": {},
   "source": [
    "# Here we preprocessing of the data for standardized ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "3f2a2831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21750000\n",
      "22050001\n",
      "22500000\n",
      "22700000\n",
      "22900000\n",
      "23100000\n",
      "23230000\n",
      "23250000\n",
      "23700000\n",
      "25200000\n",
      "26200000\n",
      "26300000\n",
      "26350000\n",
      "26800000\n",
      "27500000\n",
      "27550000\n",
      "28240000\n",
      "28300000\n",
      "28850000\n",
      "29050000\n",
      "29100000\n",
      "29200000\n"
     ]
    }
   ],
   "source": [
    "for site_no in station_df.site_no:\n",
    "    station_url = f'az://{container}/{site_no}.csv'\n",
    "    station_url2 = f'az://{container}/{site_no}_2.csv'\n",
    "    site_df1_raw = pd.read_csv(station_url, delimiter=';', skiprows=10, storage_options=storage_options)\n",
    "    translation = pd.read_csv(f'az://{container}/ana_translations.csv', storage_options=storage_options)\n",
    "    trans = {p:e  for p,e in zip(translation.Portuguese, translation.English)}\n",
    "    site_df1 = site_df1_raw.rename(columns=trans)\n",
    "    site_df1 = site_df1.dropna(subset=['Date'])\n",
    "    site_df1['TimeL'] = site_df1['TimeL'].fillna('01/01/1900 01:00')\n",
    "    site_df1['Date-Time'] = [f\"{d} {t.split(' ')[1]}\" for d,t in zip(site_df1['Date'],site_df1['TimeL'])]\n",
    "    site_df1['Date-Time'] = pd.to_datetime(site_df1['Date-Time'])\n",
    "\n",
    "    site_df2_raw = pd.read_csv(station_url2, delimiter=';', skiprows=14, storage_options=storage_options)\n",
    "    site_df2_raw = site_df2_raw.replace('01/01/1900', '01/01/1900 01:00')\n",
    "    translation2 = {'Data':'Date','Hora':'Hour','Turbidez':'Turbidity'}\n",
    "    site_df2 = site_df2_raw.rename(columns=translation2)\n",
    "    site_df2 = site_df2.dropna(subset=['Date'])\n",
    "    site_df2['Date-Time'] = [f\"{d} {t.split(' ')[1]}\" for d,t in zip(site_df2['Date'],site_df2['Hour'])]\n",
    "    site_df2['Date-Time'] = pd.to_datetime(site_df2['Date-Time'])\n",
    "    site_df2 = site_df2[['Date', 'Hour', 'Date-Time','Turbidity']]\n",
    "\n",
    "    selection = ['Date-Time', 'Discharge', 'Suspended Sediment Concentration (mg/L)', 'Turbidity']\n",
    "    site_df = site_df1.merge(site_df2, on='Date', how='outer', suffixes=('_',''))\n",
    "    site_df['Date-Time'] = site_df['Date-Time'].fillna(site_df['Date-Time_'])\n",
    "    #site_df['Hour'] = site_df['Hour'].fillna(site_df['Hour_'])\n",
    "    site_df = site_df[selection]\n",
    "\n",
    "    write_filename = f'az://{container}/stations/{site_no}.csv'\n",
    "    site_df.to_csv(write_filename, index=False, storage_options=storage_options)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
