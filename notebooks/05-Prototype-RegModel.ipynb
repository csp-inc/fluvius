{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049f010e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-0.24.2-cp38-cp38-manylinux2010_x86_64.whl (24.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.9 MB 12.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sklearn) (1.19.5)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading joblib-1.0.1-py3-none-any.whl (303 kB)\n",
      "\u001b[K     |████████████████████████████████| 303 kB 74.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sklearn) (1.6.3)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1316 sha256=2fd7e58442a1c424caea62eda204f3385305ceafd3d32ab978d1d72d5f995fb2\n",
      "  Stored in directory: /root/.cache/pip/wheels/22/0b/40/fd3f795caaa1fb4c6cb738bc1f56100be1e57da95849bfc897\n",
      "Successfully built sklearn\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn, sklearn\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1eb18555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "### Environment setup\n",
    "import sys\n",
    "sys.path.append('/content')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from pystac_client import Client\n",
    "import planetary_computer as pc\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env_vars = !cat /content/.env\n",
    "\n",
    "for var in env_vars:\n",
    "    key, value = var.split(' = ')\n",
    "    os.environ[key] = value\n",
    "\n",
    "storage_options={'account_name':os.environ['ACCOUNT_NAME'],\\\n",
    "                 'account_key':os.environ['BLOB_KEY']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a3476ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:06<00:00, 3114.79it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "### Load the data\n",
    "\n",
    "#d = pd.read_csv(\"https://donaldpinckney.com/books/pytorch/book/ch2-linreg/code/linreg-multi-synthetic-2.csv\", header=None)\n",
    "d = pd.read_csv(\"az://modeling-data/fluvius_data.csv\", storage_options=storage_options)\n",
    "d = d[d['Chip Cloud Pct']<20]\n",
    "ds = d[['sentinel-2-l2a_R','sentinel-2-l2a_G','sentinel-2-l2a_B','julian_date','SSC (mg/L)']]\n",
    "D = torch.tensor(ds.values, dtype=torch.float)\n",
    "\n",
    "dims = 4\n",
    "# We extract all rows and the first 4 columns, and then transpose it\n",
    "x_dataset = D[:, 0:dims].t()\n",
    "\n",
    "# We extract all rows and the last column, and transpose it\n",
    "y_dataset = D[:, dims].t()\n",
    "\n",
    "# And make a convenient variable to remember the number of input columns\n",
    "n = dims \n",
    "\n",
    "### Feature Scaling computations\n",
    "\n",
    "# Pre-compute the means and standard deviations of independent variables\n",
    "means = x_dataset.mean(1, keepdim=True)\n",
    "deviations = x_dataset.std(1, keepdim=True)\n",
    "\n",
    "### Model definition ###\n",
    "\n",
    "# First we define the trainable parameters A and b \n",
    "A = torch.randn((1, n), requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "\n",
    "# Then we define the prediction model\n",
    "#def model(x_input):\n",
    "#    return A.mm(x_input) + b\n",
    "\n",
    "# Then we define the prediction model\n",
    "def model(x_input):\n",
    "    x_transformed = (x_input - means) / deviations\n",
    "    return A.mm(x_transformed) + b\n",
    "\n",
    "### Loss function definition ###\n",
    "\n",
    "def loss(y_predicted, y_target):\n",
    "    return ((y_predicted - y_target)**2).sum()\n",
    "### Training the model ###\n",
    "\n",
    "# Setup the optimizer object, so it optimizes a and b.\n",
    "optimizer = optim.Adam([A, b], lr=0.01)\n",
    "\n",
    "# Main optimization loop\n",
    "for t in tqdm(range(20000)):\n",
    "    # Set the gradients to 0.\n",
    "    optimizer.zero_grad()\n",
    "    # Compute the current predicted y's from x_dataset\n",
    "    y_predicted = model(x_dataset)\n",
    "    # See how far off the prediction is\n",
    "    current_loss = loss(y_predicted, y_dataset)\n",
    "    # Compute the gradient of the loss with respect to A and b.\n",
    "    current_loss.backward()\n",
    "    # Update A and b accordingly.\n",
    "    optimizer.step()\n",
    "    #print(f\"t = {t}, loss = {current_loss}, A = {A.detach().numpy()}, b = {b.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0d0288db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25049799665750605"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_true=np.expand_dims(y_dataset.numpy(),axis=0)[0], y_pred=y_predicted.detach().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259bf17d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
