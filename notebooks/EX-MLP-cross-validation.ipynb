{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"/content\")\n",
    "from src.utils import fit_mlp, plot_obs_predict\n",
    "\n",
    "with open(\"/content/credentials\") as f:\n",
    "    env_vars = f.read().split(\"\\n\")\n",
    "\n",
    "for var in env_vars:\n",
    "    key, value = var.split(\" = \")\n",
    "    os.environ[key] = value\n",
    "\n",
    "storage_options = {\"account_name\":os.environ[\"ACCOUNT_NAME\"],\n",
    "                   \"account_key\":os.environ[\"BLOB_KEY\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify hyperparameters and fit model\n",
    "buffer_distance = 500\n",
    "day_tolerance = 8\n",
    "cloud_thr = 80\n",
    "min_water_pixels = 10\n",
    "features = [\n",
    "    \"sentinel-2-l2a_AOT\", \n",
    "    \"sentinel-2-l2a_B02\", \"sentinel-2-l2a_B03\", \"sentinel-2-l2a_B04\", # RGB bands\n",
    "    \"sentinel-2-l2a_B08\", # NIR\n",
    "    #\"sentinel-2-l2a_WVP\", \n",
    "    \"sentinel-2-l2a_B05\", \"sentinel-2-l2a_B06\", \"sentinel-2-l2a_B07\", \"sentinel-2-l2a_B8A\",  # Red edge bands\n",
    "    \"is_brazil\", #\"sine_julian\", \n",
    "    \"sentinel-2-l2a_B11\", \"sentinel-2-l2a_B12\", # SWIR\n",
    "    \"mean_viewing_azimuth\", \"mean_viewing_zenith\",\n",
    "    \"mean_solar_azimuth\", \"mean_solar_zenith\"\n",
    "]\n",
    "epochs = 1000\n",
    "batch_size = 32\n",
    "learning_rate = 0.005\n",
    "\n",
    "layer_out_neurons = [12, 24, 6]\n",
    "\n",
    "buffer_distance = 500\n",
    "day_tolerance = 8\n",
    "cloud_thr = 80\n",
    "mask_method1 = \"lulc\"\n",
    "mask_method2 = \"mndwi\"\n",
    "min_water_pixels = 1\n",
    "layer_out_neurons = [24, 12, 6]\n",
    "learn_sched_step_size = 200 \n",
    "learn_sched_gamma = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "n_layers = len(layer_out_neurons)\n",
    "\n",
    "# Read the data\n",
    "fp = f\"/content/local/partitioned_feature_data_buffer500m_daytol8_cloudthr80percent_lulcmndwi_masking_tmp.csv\"\n",
    "data = pd.read_csv(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data[\"Log SSC (mg/L)\"] = np.log(data[\"SSC (mg/L)\"])\n",
    "response = \"Log SSC (mg/L)\"\n",
    "\n",
    "lnssc_0 = data[\"Log SSC (mg/L)\"] == 0\n",
    "data.drop(lnssc_0[lnssc_0].index, inplace=True)\n",
    "not_enough_water = data[\"n_water_pixels\"] < 10\n",
    "data.drop(not_enough_water[not_enough_water].index, inplace=True)\n",
    "data[\"fold\"] = random.choices([1,2,3,4,5], k=data.shape[0])\n",
    "data[\"partition\"]\n",
    "data.drop(data[data[\"partition\"] == \"testing\"].index, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training.\n",
      "Epoch 50/1000 | Train Loss: 0.52757\n",
      "Epoch 100/1000 | Train Loss: 0.44099\n",
      "Epoch 150/1000 | Train Loss: 0.30658\n",
      "Epoch 200/1000 | Train Loss: 0.28053\n",
      "Epoch 250/1000 | Train Loss: 0.26680\n",
      "Epoch 300/1000 | Train Loss: 0.26705\n",
      "Epoch 350/1000 | Train Loss: 0.25177\n",
      "Epoch 400/1000 | Train Loss: 0.25805\n",
      "Epoch 450/1000 | Train Loss: 0.24883\n",
      "Epoch 500/1000 | Train Loss: 0.24426\n",
      "Epoch 550/1000 | Train Loss: 0.25567\n",
      "Epoch 600/1000 | Train Loss: 0.24904\n",
      "Epoch 650/1000 | Train Loss: 0.24231\n",
      "Epoch 700/1000 | Train Loss: 0.24394\n",
      "Epoch 750/1000 | Train Loss: 0.24064\n",
      "Epoch 800/1000 | Train Loss: 0.25758\n",
      "Epoch 850/1000 | Train Loss: 0.24109\n",
      "Epoch 900/1000 | Train Loss: 0.24642\n",
      "Epoch 950/1000 | Train Loss: 0.24290\n",
      "Epoch 1000/1000 | Train Loss: 0.24305\n",
      "Begin training.\n",
      "Epoch 50/1000 | Train Loss: 0.33953\n",
      "Epoch 100/1000 | Train Loss: 0.29922\n",
      "Epoch 150/1000 | Train Loss: 0.25759\n",
      "Epoch 200/1000 | Train Loss: 0.25191\n",
      "Epoch 250/1000 | Train Loss: 0.23287\n",
      "Epoch 300/1000 | Train Loss: 0.22995\n",
      "Epoch 350/1000 | Train Loss: 0.22701\n",
      "Epoch 400/1000 | Train Loss: 0.22622\n",
      "Epoch 450/1000 | Train Loss: 0.21792\n",
      "Epoch 500/1000 | Train Loss: 0.22015\n",
      "Epoch 550/1000 | Train Loss: 0.22005\n",
      "Epoch 600/1000 | Train Loss: 0.21919\n",
      "Epoch 650/1000 | Train Loss: 0.22006\n",
      "Epoch 700/1000 | Train Loss: 0.21912\n",
      "Epoch 750/1000 | Train Loss: 0.21923\n",
      "Epoch 800/1000 | Train Loss: 0.21472\n",
      "Epoch 850/1000 | Train Loss: 0.21481\n",
      "Epoch 900/1000 | Train Loss: 0.21789\n",
      "Epoch 950/1000 | Train Loss: 0.21303\n",
      "Epoch 1000/1000 | Train Loss: 0.21926\n",
      "Begin training.\n",
      "Epoch 50/1000 | Train Loss: 0.41391\n",
      "Epoch 100/1000 | Train Loss: 0.39680\n",
      "Epoch 150/1000 | Train Loss: 0.34565\n",
      "Epoch 200/1000 | Train Loss: 0.33618\n",
      "Epoch 250/1000 | Train Loss: 0.32254\n",
      "Epoch 300/1000 | Train Loss: 0.31572\n",
      "Epoch 350/1000 | Train Loss: 0.30644\n",
      "Epoch 400/1000 | Train Loss: 0.30112\n",
      "Epoch 450/1000 | Train Loss: 0.29833\n",
      "Epoch 500/1000 | Train Loss: 0.29716\n",
      "Epoch 550/1000 | Train Loss: 0.29869\n",
      "Epoch 600/1000 | Train Loss: 0.29899\n",
      "Epoch 650/1000 | Train Loss: 0.29511\n",
      "Epoch 700/1000 | Train Loss: 0.29460\n",
      "Epoch 750/1000 | Train Loss: 0.29410\n",
      "Epoch 800/1000 | Train Loss: 0.29443\n",
      "Epoch 850/1000 | Train Loss: 0.29391\n",
      "Epoch 900/1000 | Train Loss: 0.29390\n",
      "Epoch 950/1000 | Train Loss: 0.29401\n",
      "Epoch 1000/1000 | Train Loss: 0.29397\n",
      "Begin training.\n",
      "Epoch 50/1000 | Train Loss: 0.39880\n",
      "Epoch 100/1000 | Train Loss: 0.31121\n",
      "Epoch 150/1000 | Train Loss: 0.28417\n",
      "Epoch 200/1000 | Train Loss: 0.25426\n",
      "Epoch 250/1000 | Train Loss: 0.23782\n",
      "Epoch 300/1000 | Train Loss: 0.23376\n",
      "Epoch 350/1000 | Train Loss: 0.22748\n",
      "Epoch 400/1000 | Train Loss: 0.22608\n",
      "Epoch 450/1000 | Train Loss: 0.22267\n",
      "Epoch 500/1000 | Train Loss: 0.22053\n",
      "Epoch 550/1000 | Train Loss: 0.22378\n",
      "Epoch 600/1000 | Train Loss: 0.22096\n",
      "Epoch 650/1000 | Train Loss: 0.21698\n",
      "Epoch 700/1000 | Train Loss: 0.21911\n",
      "Epoch 750/1000 | Train Loss: 0.22029\n",
      "Epoch 800/1000 | Train Loss: 0.21851\n",
      "Epoch 850/1000 | Train Loss: 0.21668\n",
      "Epoch 900/1000 | Train Loss: 0.21862\n",
      "Epoch 950/1000 | Train Loss: 0.21958\n",
      "Epoch 1000/1000 | Train Loss: 0.21813\n",
      "Begin training.\n",
      "Epoch 50/1000 | Train Loss: 0.42106\n",
      "Epoch 100/1000 | Train Loss: 0.38783\n",
      "Epoch 150/1000 | Train Loss: 0.38174\n",
      "Epoch 200/1000 | Train Loss: 0.35485\n",
      "Epoch 250/1000 | Train Loss: 0.34418\n",
      "Epoch 300/1000 | Train Loss: 0.34207\n",
      "Epoch 350/1000 | Train Loss: 0.32381\n",
      "Epoch 400/1000 | Train Loss: 0.31255\n",
      "Epoch 450/1000 | Train Loss: 0.30507\n",
      "Epoch 500/1000 | Train Loss: 0.30265\n",
      "Epoch 550/1000 | Train Loss: 0.30032\n",
      "Epoch 600/1000 | Train Loss: 0.29874\n",
      "Epoch 650/1000 | Train Loss: 0.29784\n",
      "Epoch 700/1000 | Train Loss: 0.29788\n",
      "Epoch 750/1000 | Train Loss: 0.29719\n",
      "Epoch 800/1000 | Train Loss: 0.29721\n",
      "Epoch 850/1000 | Train Loss: 0.29678\n",
      "Epoch 900/1000 | Train Loss: 0.29653\n",
      "Epoch 950/1000 | Train Loss: 0.29641\n",
      "Epoch 1000/1000 | Train Loss: 0.29671\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(data[features])\n",
    "val_mse = []\n",
    "fold_n_sites = []\n",
    "\n",
    "for fold in [1,2,3,4,5]:\n",
    "    X_train = X_scaled[data[\"fold\"] != fold]\n",
    "    y_train = data[data[\"fold\"] != fold][response]\n",
    "    X_val = X_scaled[data[\"fold\"] == fold]\n",
    "    y_val = data[data[\"fold\"] == fold][response]\n",
    "    site_val = data[data[\"fold\"] == fold][\"site_no\"]\n",
    "    X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "    X_val, y_val = np.array(X_val), np.array(y_val)\n",
    "\n",
    "    class RegressionDataset(Dataset):\n",
    "\n",
    "        def __init__(self, X_data, y_data):\n",
    "            self.X_data = X_data\n",
    "            self.y_data = y_data\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            return self.X_data[index], self.y_data[index]\n",
    "\n",
    "        def __len__ (self):\n",
    "            return len(self.X_data)\n",
    "\n",
    "    train_dataset = RegressionDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).float())\n",
    "    val_dataset = RegressionDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val).float())\n",
    "\n",
    "    num_features = X_train.shape[1]\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    train_loader_all = DataLoader(dataset=train_dataset, batch_size=1)\n",
    "    val_loader = DataLoader(dataset=val_dataset, batch_size=1)\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    global MultipleRegression\n",
    "    class MultipleRegression(nn.Module):\n",
    "        def __init__(self, num_features, n_layers, layer_out_neurons):\n",
    "            super(MultipleRegression, self).__init__()\n",
    "            self.n_layers = n_layers\n",
    "            self.layer_out_neurons = layer_out_neurons\n",
    "\n",
    "            most_recent_n_neurons = layer_out_neurons[0]\n",
    "            self.layer_1 = nn.Linear(num_features, layer_out_neurons[0])\n",
    "\n",
    "            for i in range(2, n_layers + 1):\n",
    "                setattr(\n",
    "                    self,\n",
    "                    f\"layer_{i}\",\n",
    "                    nn.Linear(layer_out_neurons[i-2], layer_out_neurons[i-1])\n",
    "                )\n",
    "                most_recent_n_neurons = layer_out_neurons[i-1]\n",
    "\n",
    "            self.layer_out = nn.Linear(most_recent_n_neurons, 1)\n",
    "            self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "        def forward(self, inputs):\n",
    "            x = self.relu(self.layer_1(inputs))\n",
    "            for i in range(2, self.n_layers + 1):\n",
    "                x = self.relu(getattr(self, f\"layer_{i}\")(x))\n",
    "\n",
    "            x = self.layer_out(x)\n",
    "\n",
    "            return (x)\n",
    "\n",
    "\n",
    "    model = MultipleRegression(num_features, n_layers, layer_out_neurons)\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.StepLR(\n",
    "        optimizer,\n",
    "        step_size=learn_sched_step_size,\n",
    "        gamma=learn_sched_gamma\n",
    "    )\n",
    "\n",
    "    loss_stats = {\n",
    "        \"train\": [],\n",
    "        \"val\": []\n",
    "    }\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Begin training.\")\n",
    "    for e in range(1, epochs+1):\n",
    "        # TRAINING\n",
    "        train_epoch_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        for X_train_batch, y_train_batch in train_loader:\n",
    "            # grab data to iteration and send to CPU\n",
    "            X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)\n",
    "\n",
    "            def closure():\n",
    "                # Zero gradients\n",
    "                optimizer.zero_grad()\n",
    "                # Forward pass\n",
    "                y_train_pred = model(X_train_batch)\n",
    "                # Compute loss\n",
    "                train_loss = criterion(y_train_pred, y_train_batch.unsqueeze(1))\n",
    "                # Backward pass\n",
    "                train_loss.backward()\n",
    "\n",
    "                return train_loss\n",
    "\n",
    "            # Update weights\n",
    "            optimizer.step(closure)\n",
    "\n",
    "            # Update the running loss\n",
    "            train_loss = closure()\n",
    "            train_epoch_loss += train_loss.item()\n",
    "\n",
    "        loss_stats[\"train\"].append(train_epoch_loss/len(train_loader))\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if (e % 50 == 0):\n",
    "            print(f\"Epoch {e}/{epochs} | Train Loss: {train_epoch_loss/len(train_loader):.5f}\")\n",
    "\n",
    "    val_pred_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for X_batch, _ in val_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_pred = model(X_batch).cpu().squeeze().tolist()#.numpy()\n",
    "            val_pred_list.append(y_pred)\n",
    "\n",
    "    val_pred = np.array(val_pred_list)\n",
    "    val_se = list((val_pred - y_val)**2)\n",
    "\n",
    "    se_df = pd.DataFrame({\n",
    "        \"se\": val_se,\n",
    "        \"site\": site_val\n",
    "    })\n",
    "\n",
    "    val_mse.append(np.mean(np.array(se_df.groupby(\"site\").mean())))\n",
    "    fold_n_sites.append(len(np.unique(site_val)))\n",
    "\n",
    "\n",
    "\n",
    "output = {\n",
    "    \"buffer_distance\": buffer_distance,\n",
    "    \"day_tolerance\": day_tolerance,\n",
    "    \"cloud_thr\": cloud_thr,\n",
    "    \"min_water_pixels\": min_water_pixels,\n",
    "    \"features\": features,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"layer_out_neurons\": layer_out_neurons,\n",
    "    \"epochs\": epochs,\n",
    "    \"val_mse\": np.average(val_mse, weights=fold_n_sites)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6938127389359943"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"val_mse\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19, 19, 17, 18, 19]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
